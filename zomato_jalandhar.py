# -*- coding: utf-8 -*-
"""Zomato Jalandhar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVHZcmqwijYOENHAU1Sbz63tazkTFJsP
"""

import requests
from bs4 import BeautifulSoup

# url = 'https://www.zomato.com/jalandhar/sehgal-fast-food-shastri-nagar'
# headers = {'User-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Mobile Safari/537.36'}
# response = requests.get(url, headers=headers)
# response.status_code

import pandas as pd

headers = {
    'User-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Mobile Safari/537.36'}

# result_container = soup.find_all('a' )
# result_container

# a = soup.find('section',{'class':'sc-iGrrsa'}).find_all('span')
# a

# soup.find_all('a',{'class':'sc-ks3f96-0'})[4].get_text().split('/')[0]

# soup.find_all('a',{'class':'sc-ks3f96-0'})[3].get_text().split('/')[0]

# soup.find_all('div',{'class':'sc-1q7bklc-8'})[0].get_text().split(' ')[0]

import pandas as pd

jal = pd.read_excel("C:\\Users\\Lenovo\\Downloads\\Jalandhar.xlsx", sheet_name="Sheet3")
jal

s = jal["Location url"]
b = s[642]
c = s[0:2]
c


def fun(x):
    x = x.replace("/order", "")
    return x


jal["Location url"] = jal["Location url"].apply(fun)
jal["reviews"] = jal["reviews"].astype(str)
jal.head()

lst = []
for i in s:
    url = i
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    a = soup.find_all('a', {'class': 'sc-ks3f96-0'})[4].get_text().split('/')[0]
    print(a)
    lst.append(a)

loc = lst
len(lst)

reviews = []
url = "https://www.zomato.com/jalandhar/biryani-by-kilo-urban-estate"
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')
print(soup.find('section', {'class': 'sc-cqCuEk'}).get_text().split('fill')[2].split(' ')[0])
soup.find_all('div', {'class': 'sc-1q7bklc-8'})[1].get_text().split(' ')[0]

lst2 = []
import requests
from bs4 import BeautifulSoup

headers = {
    'User-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Mobile Safari/537.36'}
s = jal["Location url"]
cnt = 0
reviews = jal["reviews"]

for i in s:
    url = i
    print(cnt, " - ", url)
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    if reviews[cnt] != "New":
        a = soup.find('section', {'class': 'sc-cqCuEk'}).get_text().split('fill')[2].split(' ')[0]
        lst2.append(a)
        print(a)
    else:
        lst2.append("0")
        print("0")
    cnt += 1

# jal["reviews"].dtype

# cnt=240
# url=s[241]
# print(cnt," - ",url)
# cnt+=1
# response = requests.get(url, headers=headers)
# soup = BeautifulSoup(response.content, 'html.parser')
# print(reviews[cnt])
# if reviews[cnt]=='New':
#     lst2.append("0")
#     print("0")
# elif reviews[cnt]!="New":
#     a = soup.find('section',{'class':'sc-cqCuEk'}).get_text().split('fill')[2].split(' ')[0]
#     lst2.append(a)
#     print(a)

jal.iloc[485]

len(lst2)

lst2[496]

# cnt=497
# reviews = jal["reviews"]
# s2 = s[497:]

# for i in s2:
#     url=i
#     print(cnt," - ",url)
#     response = requests.get(url, headers=headers)
#     soup = BeautifulSoup(response.content, 'html.parser')
#     if reviews[cnt]!="New":
#         a = soup.find('section',{'class':'sc-cqCuEk'}).get_text().split('fill')[2].split(' ')[0]
#         lst2.append(a)
#         print(a)
#     else:
#         lst2.append("0")
#         print("0")
#     cnt+=1


s[520]

len(lst2)

lst3 = lst2[0:496]
len(lst3)

lst2[520]

jal["location"] = lst
jal["no. of reviews"] = lst2
jal.head()

jal["cuisine"].value_counts()

jal.sample(20)

jal.to_excel("ashu_zomato_jalandhar.xlsx")

a = jal

a = a.assign(cuisine=a['cuisine'].str.split(',')).explode('cuisine')

a

a["cuisine"].value_counts()

a.isnull().sum()

jal = a
jal.to_csv("jalandhar_zomato.csv")

a["location"].value_counts()
a

temp = a
temp.head(10)

temp.drop(columns=["name", "Location url"], inplace=True)


def fun(x):
    x = x.replace("?", "")
    x = x.replace(" for one", "")
    return x


temp["price"] = temp["price"].astype(str)
temp["price"] = temp["price"].apply(fun)
temp["price"] = temp["price"].astype(float)
temp.head()

temp["no. of reviews"] = temp["no. of reviews"].astype(str)


def fun2(x):
    x = x.replace(",", "")
    return x


temp["no. of reviews"] = temp["no. of reviews"].apply(fun2)
temp.head()


def fun3(x):
    if 'K' in x:
        x = x.replace("K", "")
        x = x.split(".")[0]
        x += '000'
    return x


fun3("40.5K")
temp["no. of reviews"] = temp["no. of reviews"].apply(fun3)
temp.head()

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()
temp['cuisine'] = label_encoder.fit_transform(temp['cuisine'])
temp['cuisine'].unique()
temp['location'] = label_encoder.fit_transform(temp['location'])
temp['location'].unique()

temp.isna().sum()
temp = temp.dropna()

temp['cuisine'].unique()

test = temp[temp["reviews"] == 'New']
train = temp[temp["reviews"] != 'New']


def dot(x):
    x = x.replace("-", "0")
    return x


# test["reviews"] = test["reviews"].astype(str)
# test["reviews"] = test["reviews"].apply(dot)
# test["reviews"] = test["reviews"].astype(float)
train["reviews"] = train["reviews"].astype(str)
train["reviews"] = train["reviews"].apply(dot)
# train["reviews"] = train["reviews"].replace("",)

train["reviews"] = train["reviews"].astype(float)

print(train["reviews"].value_counts())
train["no. of reviews"] = train["no. of reviews"].astype(float)
train["reviews"] = train["reviews"].replace(0, train["reviews"].mean())
train

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(train.drop(columns=["reviews"]), train["reviews"])

model.predict([[100, 74, 9, 50000]])

train["no. of reviews"].dtype

import statsmodels.api as smf

X = smf.add_constant(train.drop(columns=["reviews"]))
lm = smf.OLS(train["reviews"], train.drop(columns=["reviews"])).fit()
lm.summary()

model.score(train.drop(columns=["reviews"]), train["reviews"])

from sklearn.tree import DecisionTreeRegressor

model2 = DecisionTreeRegressor(max_depth=3, criterion='mse')
model2.fit(train.drop(columns=["reviews"]), train["reviews"])

model2.score(train.drop(columns=["reviews"]), train["reviews"])

model2.fit()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train.drop(columns=["reviews"]), train["reviews"], test_size=0.20,
                                                    random_state=10)

model2.fit(X_train, y_train)

model2.score(X_test, y_test)

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestRegressor

mod = RandomForestRegressor(n_estimators=15, random_state=13)
mod.fit(X_train, y_train)

mod.score(X_test, y_test)

mod.score(X_train, y_train)

mod.predict([[100, ]])

mod.predict([[100, 3, 8, 450]]).round(2)

from sklearn.svm import SVR

regressor = SVR()
regressor.fit(X_train, y_train)
regressor.score(X_test, y_test)
regressor.score(X_train, y_train)

from sklearn.model_selection import GridSearchCV

param_grid = {'bootstrap': [True], 'max_depth': [5, 10, None], 'max_features': ['auto', 'log2'],
              'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}

rfr = RandomForestRegressor(random_state=1)

g_search = GridSearchCV(estimator=rfr, param_grid=param_grid,
                        cv=3, n_jobs=1, verbose=0, return_train_score=True)

g_search.fit(X_train, y_train)
print(g_search.best_params_)

print(g_search.score(X_test, y_test))

print(g_search.score(X_train, y_train))

import numpy as np

from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start=5, stop=15, num=10)]  # returns 10 numbers

max_features = ['auto', 'log2']

max_depth = [int(x) for x in np.linspace(5, 10, num=2)]

max_depth.append(None)

bootstrap = [True, False]

r_grid = {'n_estimators': n_estimators,

          'max_features': max_features,

          'max_depth': max_depth,

          'bootstrap': bootstrap}

print(r_grid)

rfr_random = RandomizedSearchCV(estimator=rfr, param_distributions=r_grid, n_iter=20, cv=3, verbose=2, random_state=42,
                                n_jobs=-1, return_train_score=True)

rfr_random.fit(X_train, y_train);

print(rfr_random.score(X_test, y_test))

print(rfr_random.score(X_train, y_train))

from sklearn.preprocessing import StandardScaler

norm_pay = StandardScaler().fit(X_train)
X_train_norm = norm_pay.transform(X_train)
X_test_norm = norm_pay.transform(X_test)

mod.fit(X_train_norm, y_train)

mod.score(X_train_norm, y_train)

mod.score(X_test_norm, y_test)

jal['cuisine']

jal.assign(cuisine=jal['cuisine'].str.split(',')).explode('cuisine')

jal[jal['cuisine'] == 'Beverage']

jal.assign(cuisine=jal['cuisine'].str.split(',')).explode('cuisine')["cuisine"].unique()

import pandas as pd
import json

df = pd.read_csv(
    "C:\\Users\\Lenovo\\Data Science\\Zomato_Github_Download\\Zomato_Restaurant_Rating_Prediction-main/jalandhar_zomato.csv")
df

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

le.fit(df["City"])

df["City"] = le.fit_transform(df["City"])
label_encoding_dict = dict()
for num, label in enumerate(le.classes_):
    label_encoding_dict[label] = num
print(json.dumps(label_encoding_dict, indent=4))

city_d = json.dumps(label_encoding_dict, indent=4)
print(city_d)

s = le.fit_transform(df["cuisine"])
label_encoding_dict = dict()
for num, label in enumerate(le.classes_):
    label_encoding_dict[label] = num
print(json.dumps(label_encoding_dict, indent=4))

s = le.fit_transform(df["location"])
label_encoding_dict = dict()
for num, label in enumerate(le.classes_):
    label_encoding_dict[label] = num
print(json.dumps(label_encoding_dict, indent=4))

"""# More relevant part """

temp = df

temp.drop(columns=["name", "Location url"], inplace=True)


def fun(x):
    x = x.replace("?", "")
    x = x.replace(" for one", "")
    return x


temp["price"] = temp["price"].astype(str)
temp["price"] = temp["price"].apply(fun)
temp["price"] = temp["price"].astype(float)
temp.head()

temp["no. of reviews"] = temp["no. of reviews"].astype(str)


def fun2(x):
    x = x.replace(",", "")
    return x


temp["no. of reviews"] = temp["no. of reviews"].apply(fun2)
temp.head()


def fun3(x):
    if 'K' in x:
        x = x.replace("K", "")
        x = x.split(".")[0]
        x += '000'
    return x


fun3("40.5K")
temp["no. of reviews"] = temp["no. of reviews"].apply(fun3)
temp.head()

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()
temp['cuisine'] = label_encoder.fit_transform(temp['cuisine'])
temp['cuisine'].unique()
temp['location'] = label_encoder.fit_transform(temp['location'])
temp['location'].unique()
temp["reviews"].unique()

temp.isna().sum()
temp = temp.dropna()

# def dot(x):
#     x = x.replace("-", "0")
#     return x

# temp[temp["reviews"]!='New'].unique()

# temp["reviews"] = temp["reviews"].astype(str)
# temp["reviews"] = temp["reviews"].apply(dot)
temp["reviews"] = temp["reviews"].replace("-", 0)
temp
# temp["reviews"] = temp["reviews"].astype(float)
temp["reviews"].unique()
temp = temp[temp["reviews"] != 'New']
temp["reviews"] = temp["reviews"].astype(float)

temp["reviews"] = temp["reviews"].replace(0, temp["reviews"].mean())
temp

print(temp["reviews"].value_counts())
temp["no. of reviews"] = temp["no. of reviews"].astype(float)
temp["reviews"] = temp["reviews"].replace(0, temp["reviews"].mean())
temp

import statsmodels.api as smf

X = smf.add_constant(temp.drop(columns=["reviews"]))
lm = smf.OLS(temp["reviews"], temp.drop(columns=["reviews"])).fit()
lm.summary()

temp.columns = ["Price", "Cuisine", "Ratings", "Location", "No. of Reviews", "City"]
temp.head()

temp = temp[['City', 'Location', 'Cuisine', 'Ratings', 'No. of Reviews', 'Price']]
temp.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(temp.drop(columns=["Ratings"]), temp["Ratings"], test_size=0.2,
                                                    random_state=10001)

from sklearn.preprocessing import StandardScaler

norm_pay = StandardScaler().fit(X_train)
X_train_norm = norm_pay.transform(X_train)
X_test_norm = norm_pay.transform(X_test)

import numpy as np

from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start=5, stop=15, num=10)]  # returns 10 numbers

max_features = ['auto', 'log2']

max_depth = [int(x) for x in np.linspace(5, 10, num=2)]

max_depth.append(None)

bootstrap = [True, False]

r_grid = {'n_estimators': n_estimators,

          'max_features': max_features,

          'max_depth': max_depth,

          'bootstrap': bootstrap}

print(r_grid)

rfr = RandomForestRegressor(random_state=1)
from sklearn.model_selection import GridSearchCV

g_search = GridSearchCV(estimator=rfr, param_grid=r_grid,
                        cv=3, n_jobs=1, verbose=0, return_train_score=True)

rfr_random = RandomizedSearchCV(estimator=rfr, param_distributions=r_grid, n_iter=20, cv=3, verbose=2, random_state=42,
                                n_jobs=-1, return_train_score=True)

rfr_random.fit(X_train, y_train);

print(rfr_random.score(X_train, y_train))
rfr_random.score(X_test, y_test)

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestRegressor

mod = RandomForestRegressor(n_estimators=300, random_state=48, max_depth=12, min_samples_leaf=3)
mod.fit(X_train, y_train)

mod.score(X_train, y_train)

mod.score(X_test, y_test)

import bz2
import pickle

with open('model_pickle', 'wb') as file:
    pickle.dump(mod, file)
# sfile = bz2.BZ2File('C:\\Users\\Lenovo\\Data Science\\Zomato_Github_Download\\Zomato_Restaurant_Rating_Prediction-main\\bestmodel\\RandomforestRegressors.pkl', 'wb')
# pickle.dump(mod, sfile)

# df = pd.read_csv("C:\\Users\\Lenovo\\Data Science\\Zomato_Github_Download\\Zomato_Restaurant_Rating_Prediction-main/jalandhar_zomato.csv")
# df

s = df["cuisine"].unique()

s.sort()
cuisine = pd.DataFrame(s)
cuisine.columns = ["Cuisine"]
cuisine.head()

# s = le.fit_transform(temp["Cuisine"])
# label_encoding_dict = dict()
# for num, label in enumerate(le.classes_):
#     label_encoding_dict[label] = num 
# cuisine_json = json.dumps(label_encoding_dict,indent=4)
# print(cuisine_json)

s = df["City"].unique()
s.sort()
city = pd.DataFrame(s)
city.columns = ["Cuisine"]
city.head()

s = le.fit_transform(df["City"])
label_encoding_dict = dict()
for num, label in enumerate(le.classes_):
    label_encoding_dict[label] = num
city_json = json.dumps(label_encoding_dict, indent=4)
print(city_json)

temp

temp.to_csv("zomato_final_data.csv")

df = pd.read_csv(
    "C:\\Users\\Lenovo\\Data Science\\Zomato_Github_Download\\Zomato_Restaurant_Rating_Prediction-main/jalandhar_zomato.csv")
df

temp.dtypes

temp.columns
